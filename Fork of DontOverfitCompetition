{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12896,"databundleVersionId":293038,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:40:35.025664Z","iopub.execute_input":"2025-10-21T11:40:35.025870Z","iopub.status.idle":"2025-10-21T11:40:37.394754Z","shell.execute_reply.started":"2025-10-21T11:40:35.025846Z","shell.execute_reply":"2025-10-21T11:40:37.391481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ydf as ydf\n\n# for KNN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# for PCA\nfrom sklearn.preprocessing import LabelEncoder # categorical variables to numbers\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# for Pair Plot and other visualisations\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:40:37.397274Z","iopub.execute_input":"2025-10-21T11:40:37.397917Z","iopub.status.idle":"2025-10-21T11:40:59.689261Z","shell.execute_reply.started":"2025-10-21T11:40:37.397882Z","shell.execute_reply":"2025-10-21T11:40:59.688388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load input data","metadata":{}},{"cell_type":"code","source":"# Load training data into a Pandas Dataframe\ndataset_df = pd.read_csv('/kaggle/input/dont-overfit-ii/train.csv')\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:40:59.690203Z","iopub.execute_input":"2025-10-21T11:40:59.691070Z","iopub.status.idle":"2025-10-21T11:40:59.741333Z","shell.execute_reply.started":"2025-10-21T11:40:59.691042Z","shell.execute_reply":"2025-10-21T11:40:59.740431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Basic initial data exploration","metadata":{}},{"cell_type":"code","source":"# Display the first 5 rows\ndataset_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:40:59.742368Z","iopub.execute_input":"2025-10-21T11:40:59.742691Z","iopub.status.idle":"2025-10-21T11:40:59.787025Z","shell.execute_reply.started":"2025-10-21T11:40:59.742668Z","shell.execute_reply":"2025-10-21T11:40:59.786087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# basic exploration of the dataset\ndataset_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:40:59.788138Z","iopub.execute_input":"2025-10-21T11:40:59.788437Z","iopub.status.idle":"2025-10-21T11:41:00.181198Z","shell.execute_reply.started":"2025-10-21T11:40:59.788414Z","shell.execute_reply":"2025-10-21T11:41:00.180389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# more basic data exploration\ndataset_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:00.184041Z","iopub.execute_input":"2025-10-21T11:41:00.184352Z","iopub.status.idle":"2025-10-21T11:41:00.214309Z","shell.execute_reply.started":"2025-10-21T11:41:00.184314Z","shell.execute_reply":"2025-10-21T11:41:00.213144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bar chart for target column\nplot_df = dataset_df.target.value_counts()\nplot_df.plot(kind=\"bar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:00.215124Z","iopub.execute_input":"2025-10-21T11:41:00.215422Z","iopub.status.idle":"2025-10-21T11:41:00.529823Z","shell.execute_reply.started":"2025-10-21T11:41:00.215392Z","shell.execute_reply":"2025-10-21T11:41:00.528979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Almost a third has value 1\nTwo thirds have value 0","metadata":{}},{"cell_type":"code","source":"# Numerical data distribution\n\n# Let us plot a random subset the numerical columns and their value counts:\n\nfig, ax = plt.subplots(5,1,  figsize=(10, 10))\nplt.subplots_adjust(top = 2)\n\nsns.histplot(dataset_df['1'], color='b', bins=50, ax=ax[0]);\nsns.histplot(dataset_df['75'], color='b', bins=50, ax=ax[1]);\nsns.histplot(dataset_df['150'], color='b', bins=50, ax=ax[2]);\nsns.histplot(dataset_df['175'], color='b', bins=50, ax=ax[3]);\nsns.histplot(dataset_df['299'], color='b', bins=50, ax=ax[4]);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:00.530756Z","iopub.execute_input":"2025-10-21T11:41:00.531078Z","iopub.status.idle":"2025-10-21T11:41:01.802450Z","shell.execute_reply.started":"2025-10-21T11:41:00.531055Z","shell.execute_reply":"2025-10-21T11:41:01.801311Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Variables 0-299 are Gaussian distributed (ok I cheated and had a look at other peoples' data exploration and comments to conclude this)","metadata":{}},{"cell_type":"markdown","source":"## Data visualisation strategies","metadata":{}},{"cell_type":"markdown","source":"To uncover patterns in the dataset try Visualization Techniques:\n1. Pair Plots\n\n    Use pair plots to visualize the relationships between multiple variables. This will help you identify correlations and distributions across pairs of variables. Libraries like Seaborn in Python can create these plots easily.\n\n2. Correlation Matrix with Heatmap\n\n    Create a correlation matrix to quantify the relationships between variables. Visualize this matrix using a heatmap to quickly identify strong correlations. This can help you understand which variables might be redundant.\n\n3. Principal Component Analysis (PCA)\n\n    Apply PCA to reduce the dimensionality of your dataset while retaining variance. Visualize the first two or three principal components in a scatter plot to see if any clusters or patterns emerge.\n\n4. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n\n    Use t-SNE for visualizing high-dimensional data in two or three dimensions. This technique can help reveal clusters or patterns that may not be apparent in the original feature space.\n\n5. Box Plots\n\n    Create box plots for each variable to visualize the distribution and identify outliers. This can provide insights into the spread and central tendency of the data.\n\n6. Violin Plots\n\n    Similar to box plots, violin plots show the distribution of the data across different categories. They can provide more information about the density of the data at different values.\n\n7. Scatter Plots with Color Coding\n\n    For pairs of variables, use scatter plots and color code the points based on the target variable (0 or 1). This can help you see how the target variable relates to the features.\n\n8. Feature Importance Visualization\n\n    If you use tree-based models (like Random Forest or Gradient Boosting), visualize feature importance to understand which variables contribute most to the predictions.\n\n9. Cumulative Distribution Function (CDF) Plots\n\n    Plot CDFs for different variables to compare their distributions. This can help you see how the distributions differ and identify any potential overlaps.\n\n10. Facet Grids\n\n    Use facet grids to create multiple plots based on the target variable. This allows you to visualize how the distributions of features differ between the two categories (0 and 1).\n","metadata":{}},{"cell_type":"markdown","source":"1. Pair Plots\n\n    Use pair plots to visualize the relationships between multiple variables. This will help you identify correlations and distributions across pairs of variables. Libraries like Seaborn in Python can create these plots easily.\n","metadata":{}},{"cell_type":"code","source":"# Pair plots\n\n# Load your dataset into a Pandas DataFrame\n# df = pd.read_csv('your_dataset.csv')  # Uncomment and modify this line to load your data\ndf = dataset_df.copy()\n\n# select a subset of variables called features\nfeatures = df.columns[df.columns != 'target']   # Exclude the target variable\n\n# Set the random seed for reproducibility\nseed = 42\n\n# Randomly select 10 columns from the DataFrame\ndf2 = df.sample(n=10, axis=1, random_state=seed)\ndf2.head()\n\n# put target back on dataframe\ndf_combined = pd.concat([df2, df['target']], axis=1)\n\n# Create pair plots\nsns.pairplot(df_combined,hue='target')\n\n# Show the plot\n#plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:01.803498Z","iopub.execute_input":"2025-10-21T11:41:01.803840Z","iopub.status.idle":"2025-10-21T11:41:36.765595Z","shell.execute_reply.started":"2025-10-21T11:41:01.803810Z","shell.execute_reply":"2025-10-21T11:41:36.764235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hmmm... No obvious correlations","metadata":{}},{"cell_type":"markdown","source":"\n2. Correlation Matrix with Heatmap\n\n    Create a correlation matrix to quantify the relationships between variables. Visualize this matrix using a heatmap to quickly identify strong correlations. This can help you understand which variables might be redundant.\n","metadata":{}},{"cell_type":"code","source":"# copy of dataset\ndf = dataset_df.copy()\n\n# Set the random seed for reproducibility\nseed = 42\n\n# Randomly select 10 columns from the DataFrame\ndf2 = df.sample(n=25, axis=1, random_state=seed)\n#df2.head()\n\n# put target back on dataframe\ndf_combined = pd.concat([df2, df['target']], axis=1)\n\n# Calculate the correlation matrix\ncorrelation_matrix = df_combined.corr()\n\n# Create a heatmap\nplt.figure(figsize=(12, 10))  # Adjust the size as needed\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n\n# Show the plot\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:36.766756Z","iopub.execute_input":"2025-10-21T11:41:36.767137Z","iopub.status.idle":"2025-10-21T11:41:38.700616Z","shell.execute_reply.started":"2025-10-21T11:41:36.767105Z","shell.execute_reply":"2025-10-21T11:41:38.699762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No obvious correlations among these variables","metadata":{}},{"cell_type":"markdown","source":"3. Principal Component Analysis (PCA)\n\n    Apply PCA to reduce the dimensionality of your dataset while retaining variance. Visualize the first two or three principal components in a scatter plot to see if any clusters or patterns emerge.","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\nX_pca = pca.fit_transform(X_scaled)\n\n# Create a DataFrame with the PCA results\npca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\npca_df['target'] = y.values  # Add the target variable for coloring\n\n# Visualize the PCA results\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='target', palette='Set1', alpha=0.7)\nplt.title('PCA of the Dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title='Target')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:38.701530Z","iopub.execute_input":"2025-10-21T11:41:38.701785Z","iopub.status.idle":"2025-10-21T11:41:39.054207Z","shell.execute_reply.started":"2025-10-21T11:41:38.701765Z","shell.execute_reply":"2025-10-21T11:41:39.053176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No obvious principle components...","metadata":{}},{"cell_type":"markdown","source":"4. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n\n    Use t-SNE for visualizing high-dimensional data in two or three dimensions. This technique can help reveal clusters or patterns that may not be apparent in the original feature space.","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)  # Reduce to 2 dimensions for visualization\nX_tsne = tsne.fit_transform(X_scaled)\n\n# Create a DataFrame with the t-SNE results\ntsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\ntsne_df['target'] = y.values  # Add the target variable for coloring\n\n# Visualize the t-SNE results\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=tsne_df, x='Dimension 1', y='Dimension 2', hue='target', palette='Set1', alpha=0.7)\nplt.title('t-SNE of the Dataset')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.legend(title='Target')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:39.055266Z","iopub.execute_input":"2025-10-21T11:41:39.055606Z","iopub.status.idle":"2025-10-21T11:41:40.337980Z","shell.execute_reply.started":"2025-10-21T11:41:39.055578Z","shell.execute_reply":"2025-10-21T11:41:40.336947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No obvious clusters","metadata":{}},{"cell_type":"markdown","source":"5. Box Plots\n\n    Create box plots for each variable to visualize the distribution and identify outliers. This can provide insights into the spread and central tendency of the data.","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Set the random seed for reproducibility\nseed = 42\n\n# Randomly select 10 columns from the DataFrame, excluding the target variable\nfeatures = df.columns[df.columns != 'target']  # Exclude the target variable\nrandom_features = df.sample(n=10, axis=1, random_state=seed)\n\n# Melt the DataFrame for easier plotting\nmelted_df = random_features.melt(var_name='Variable', value_name='Value')\n\n# Create box plots\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=melted_df, x='Variable', y='Value')\nplt.title('Box Plots of Randomly Selected Variables')\nplt.xticks(rotation=45)  # Rotate x-axis labels for better readability\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:40.339189Z","iopub.execute_input":"2025-10-21T11:41:40.339465Z","iopub.status.idle":"2025-10-21T11:41:40.624989Z","shell.execute_reply.started":"2025-10-21T11:41:40.339445Z","shell.execute_reply":"2025-10-21T11:41:40.624017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6. Violin Plots\n\n    Similar to box plots, violin plots show the distribution of the data across different categories. They can provide more information about the density of the data at different values.","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Set the random seed for reproducibility\nseed = 50\n\n# Randomly select 10 columns from the DataFrame, excluding the target variable\nfeatures = df.columns[df.columns != 'target']  # Exclude the target variable\nrandom_features = df.sample(n=10, axis=1, random_state=seed)\n\n# Melt the DataFrame for easier plotting\nmelted_df = random_features.melt(var_name='Variable', value_name='Value')\n\n# Create violin plots\nplt.figure(figsize=(12, 6))\nsns.violinplot(data=melted_df, x='Variable', y='Value', inner='quartile')\nplt.title('Violin Plots of Randomly Selected Variables')\nplt.xticks(rotation=45)  # Rotate x-axis labels for better readability\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:40.626174Z","iopub.execute_input":"2025-10-21T11:41:40.626523Z","iopub.status.idle":"2025-10-21T11:41:40.953149Z","shell.execute_reply.started":"2025-10-21T11:41:40.626495Z","shell.execute_reply":"2025-10-21T11:41:40.952283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No obvious pattern","metadata":{}},{"cell_type":"markdown","source":"7. Scatter Plots with Color Coding\n\n    For pairs of variables, use scatter plots and color code the points based on the target variable (0 or 1). This can help you see how the target variable relates to the features.","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Set the random seed for reproducibility\nseed = 4\n\n# Select continuous variables, excluding the target variable\ncontinuous_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\ncontinuous_features.remove('target')  # Exclude the target variable\n\n# Randomly select two variables called features here\nrandom_features = df.sample(n=2, axis=1, random_state=seed)\n\n# Create a scatter plot with color coding\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=random_features.iloc[:, 0], y=random_features.iloc[:, 1], hue='target', palette='Set1', alpha=0.7)\nplt.title(f'Scatter Plot of {random_features.columns[0]} vs {random_features.columns[1]}')\nplt.xlabel(random_features.columns[0])\nplt.ylabel(random_features.columns[1])\nplt.legend(title='Target')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:40.954025Z","iopub.execute_input":"2025-10-21T11:41:40.954284Z","iopub.status.idle":"2025-10-21T11:41:41.216589Z","shell.execute_reply.started":"2025-10-21T11:41:40.954266Z","shell.execute_reply":"2025-10-21T11:41:41.215725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No\nCorrelation\nAt all","metadata":{}},{"cell_type":"markdown","source":"8. Feature Importance Visualization\n\n    If you use tree-based models (like Random Forest or Gradient Boosting), visualize feature importance to understand which variables contribute most to the predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Assuming your DataFrame is named 'dataset_df'\ndf = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Get feature importances\nimportances = model.feature_importances_\n\n# Create a DataFrame for visualization\nfeature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# Visualize feature importances\nplt.figure(figsize=(12, 6))\nsns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')\nplt.title('Feature Importance Visualization')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:41.217422Z","iopub.execute_input":"2025-10-21T11:41:41.217648Z","iopub.status.idle":"2025-10-21T11:41:43.969786Z","shell.execute_reply.started":"2025-10-21T11:41:41.217631Z","shell.execute_reply":"2025-10-21T11:41:43.968656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most prominent features have importance less than 0.0125 i.e. very little importance.That is, no features stand out.","metadata":{}},{"cell_type":"markdown","source":"9. Cumulative Distribution Function (CDF) Plots\n\n    Plot CDFs for different variables to compare their distributions. This can help you see how the distributions differ and identify any potential overlaps.","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Set the random seed for reproducibility\nseed = 5\n\n# Select continuous variables, excluding the target variable\ncontinuous_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\ncontinuous_features.remove('target')  # Exclude the target variable\n\n# Randomly select 10 continuous features\nrandom_features = df[continuous_features].sample(n=10, axis=1, random_state=seed)\n\n# Create CDF plots for the randomly selected features\nplt.figure(figsize=(12, 8))\nfor feature in random_features.columns:\n    sns.ecdfplot(data=df, x=feature, label=feature)\n\nplt.title('Cumulative Distribution Function (CDF) Plots for Randomly Selected Features')\nplt.xlabel('Feature Value')\nplt.ylabel('Cumulative Probability')\nplt.legend(title='Features', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:43.970812Z","iopub.execute_input":"2025-10-21T11:41:43.971103Z","iopub.status.idle":"2025-10-21T11:41:44.349385Z","shell.execute_reply.started":"2025-10-21T11:41:43.971076Z","shell.execute_reply":"2025-10-21T11:41:44.348412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No obvious pattern, feature or anything from CDF","metadata":{}},{"cell_type":"markdown","source":"10. Facet Grids\n\n    Use facet grids to create multiple plots based on the target variable. This allows you to visualize how the distributions of features differ between the two categories (0 and 1).","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Set the random seed for reproducibility\nseed = 42\n\n# Select continuous variables, excluding the target variable\ncontinuous_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\ncontinuous_features.remove('target')  # Exclude the target variable\n\n# Randomly select 10 continuous features\nrandom_features = df[continuous_features].sample(n=3, axis=1, random_state=seed)\n\n# Create facet grids for the randomly selected features\nfor feature in random_features.columns:\n    g = sns.FacetGrid(df, hue='target', height=5, aspect=1.5)\n    g.map(sns.histplot, feature, kde=True, bins=30)\n    g.add_legend()\n    g.set_axis_labels(feature, 'Frequency')\n    g.set_titles(f'Distribution of {feature} by Target Variable')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:44.352725Z","iopub.execute_input":"2025-10-21T11:41:44.353086Z","iopub.status.idle":"2025-10-21T11:41:45.938622Z","shell.execute_reply.started":"2025-10-21T11:41:44.353066Z","shell.execute_reply":"2025-10-21T11:41:45.937551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No, absolutely no obvious pattern or anything here.","metadata":{}},{"cell_type":"markdown","source":"# Prepare data for analysis","metadata":{}},{"cell_type":"code","source":"# Drop columns that are not necessary for model training\ndataset_df = dataset_df.drop(['id', ], axis=1)\ndataset_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:45.939494Z","iopub.execute_input":"2025-10-21T11:41:45.939751Z","iopub.status.idle":"2025-10-21T11:41:45.963678Z","shell.execute_reply.started":"2025-10-21T11:41:45.939730Z","shell.execute_reply":"2025-10-21T11:41:45.962907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check for missing values","metadata":{}},{"cell_type":"code","source":"# We will check for the missing values using the following code:\ndataset_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:45.964523Z","iopub.execute_input":"2025-10-21T11:41:45.964802Z","iopub.status.idle":"2025-10-21T11:41:45.985291Z","shell.execute_reply.started":"2025-10-21T11:41:45.964781Z","shell.execute_reply":"2025-10-21T11:41:45.984380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"conclusion: No missing values","metadata":{}},{"cell_type":"markdown","source":"## Handling missing and Nan entries","metadata":{}},{"cell_type":"markdown","source":"Not relevant in this competition","metadata":{}},{"cell_type":"markdown","source":"## Convert booleans into the integer format for the package/library Tensor Flow Decision Forest TF-DF","metadata":{}},{"cell_type":"markdown","source":"Not relevant for this competition","metadata":{}},{"cell_type":"markdown","source":"## Split columns with a mix of letters/integers/Booleans into separate columns","metadata":{}},{"cell_type":"markdown","source":"Not relevant for this competition","metadata":{}},{"cell_type":"code","source":"# Remove original olumn from the dataset\n# In this case we say that the column could have been Cabin with contents Xyz123True\n\ntry:\n    dataset_df = dataset_df.drop('Cabin', axis=1)\nexcept KeyError:\n    print(\"Field does not exist\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:45.986217Z","iopub.execute_input":"2025-10-21T11:41:45.986674Z","iopub.status.idle":"2025-10-21T11:41:46.000706Z","shell.execute_reply.started":"2025-10-21T11:41:45.986644Z","shell.execute_reply":"2025-10-21T11:41:45.999687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"combine feature selection methods with XGBoost while setting the regularization parameter alpha to a conservative value","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Assuming your DataFrame is named 'dataset_df'\ndf = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize XGBoost classifier with conservative alpha value\nxgb_model = XGBClassifier(alpha=0.1, random_state=42)  # Set alpha to a conservative value\n\n# Use RFE for feature selection\nselector = RFE(estimator=xgb_model, n_features_to_select=10)  # Select top 10 features\nselector = selector.fit(X_train, y_train)\n\n# Get the selected features\nselected_features = X.columns[selector.support_]\nprint(\"Selected Features:\", selected_features)\n\n# Train the XGBoost model with selected features\nxgb_model.fit(X_train[selected_features], y_train)\n\n# Make predictions\ny_pred = xgb_model.predict(X_test[selected_features])\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:41:46.001841Z","iopub.execute_input":"2025-10-21T11:41:46.002154Z","iopub.status.idle":"2025-10-21T11:42:50.469029Z","shell.execute_reply.started":"2025-10-21T11:41:46.002125Z","shell.execute_reply":"2025-10-21T11:42:50.467730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model accuracy 0.78 is surprisingly high for random noise.\nVisualise with a ROC:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n# Compute ROC curve and ROC area\nfpr, tpr, thresholds = roc_curve(y_test, xgb_model.predict_proba(X_test[selected_features])[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:42:50.469653Z","iopub.execute_input":"2025-10-21T11:42:50.469903Z","iopub.status.idle":"2025-10-21T11:42:50.668465Z","shell.execute_reply.started":"2025-10-21T11:42:50.469883Z","shell.execute_reply":"2025-10-21T11:42:50.667536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's try again with a different seed and check if the same features are selected as above:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Assuming your DataFrame is named 'dataset_df'\ndf = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2) # chanced seed here!\n\n# Initialize XGBoost classifier with conservative alpha value\nxgb_model = XGBClassifier(alpha=0.1, random_state=42)  # Set alpha to a conservative value\n\n# Use RFE for feature selection\nselector = RFE(estimator=xgb_model, n_features_to_select=10)  # Select top 10 features\nselector = selector.fit(X_train, y_train)\n\n# Get the selected features\nselected_features = X.columns[selector.support_]\nprint(\"Selected Features:\", selected_features)\n\n# Train the XGBoost model with selected features\nxgb_model.fit(X_train[selected_features], y_train)\n\n# Make predictions\ny_pred = xgb_model.predict(X_test[selected_features])\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:42:50.669298Z","iopub.execute_input":"2025-10-21T11:42:50.669536Z","iopub.status.idle":"2025-10-21T11:43:52.672971Z","shell.execute_reply.started":"2025-10-21T11:42:50.669519Z","shell.execute_reply":"2025-10-21T11:43:52.671519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us compare selected features from seed 42 to seed 2:\n\nseed 42:\nSelected Features: Index(['3', '18', '53', '66', '69', '93', '127', '133', '176', '200'], dtype='object')\nModel Accuracy: 0.78\n\n\nseed 2:\nSelected Features: Index(['66', '95', '127', '133', '140', '145', '158', '199', '243', '298'], dtype='object')\nModel Accuracy: 0.68\n\nAs suspected, the selected features change with the seed. Feature 66 and 127 however appear in both models.\n\nMaybe this can be exploited:","metadata":{}},{"cell_type":"code","source":"df = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Number of iterations and threshold for feature selection\n# n_iterations = 100 # XXX <--- change back!!\n# n_iterations = 20 \nn_iterations = 1 # duration of 1 run order of magnitude 1 min Keeping for faster run of notebook\nselection_threshold = 0.7\n\n# Initialize a dictionary to count feature selections\nfeature_selection_count = {feature: 0 for feature in X.columns}\n\n# Repeat feature selection with different seeds\nfor seed in range(n_iterations):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    # Initialize XGBoost classifier\n    xgb_model = XGBClassifier(alpha=0.1, random_state=seed)\n\n    # Use RFE for feature selection\n    selector = RFE(estimator=xgb_model, n_features_to_select=10)  # Select top 10 features\n    selector = selector.fit(X_train, y_train)\n\n    # Update the count for selected features\n    for feature in X.columns[selector.support_]:\n        feature_selection_count[feature] += 1\n\n# Filter features based on the selection threshold\nselected_features = [feature for feature, count in feature_selection_count.items() if count / n_iterations >= selection_threshold]\n\nprint(\"Consistently Selected Features:\", selected_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:43:52.673805Z","iopub.execute_input":"2025-10-21T11:43:52.674058Z","iopub.status.idle":"2025-10-21T11:44:54.997305Z","shell.execute_reply.started":"2025-10-21T11:43:52.674036Z","shell.execute_reply":"2025-10-21T11:44:54.995935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Consistently Selected Features: ['127'] \nhmmm 1 feature from 20 iterations is not a lot.","metadata":{}},{"cell_type":"markdown","source":"Let us explore polynomial combinations of existing features:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ndf = dataset_df.copy()\n\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\n\n# Initialize PolynomialFeatures\ndegree = 2  # Specify the degree of the polynomial features\npoly = PolynomialFeatures(degree=degree, include_bias=False)\n\n# Generate polynomial features\nX_poly = poly.fit_transform(df)\n\n# Create a DataFrame with the new polynomial features\npoly_feature_names = poly.get_feature_names_out(input_features=df.columns)\ndf_poly = pd.DataFrame(X_poly, columns=poly_feature_names)\n\n# Display the original and polynomial features\nprint(\"Original Features:\")\nprint(df)\nprint(\"\\nPolynomial Features:\")\nprint(df_poly)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:44:54.997994Z","iopub.execute_input":"2025-10-21T11:44:54.998457Z","iopub.status.idle":"2025-10-21T11:44:55.638785Z","shell.execute_reply.started":"2025-10-21T11:44:54.998429Z","shell.execute_reply":"2025-10-21T11:44:55.637742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Statistical Summary of original and polynomial features takes too long - aborted:","metadata":{}},{"cell_type":"code","source":"# Statistical summary of original features\n#original_summary = df.describe()\n\n# Statistical summary of polynomial features\n#polynomial_summary = df_poly.describe()\n\n#print(\"Original Features Summary:\")\n#print(original_summary)\n\n#print(\"\\nPolynomial Features Summary:\")\n#print(polynomial_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:44:55.640031Z","iopub.execute_input":"2025-10-21T11:44:55.640362Z","iopub.status.idle":"2025-10-21T11:44:55.644370Z","shell.execute_reply.started":"2025-10-21T11:44:55.640313Z","shell.execute_reply":"2025-10-21T11:44:55.643246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"correlation matrix for both sets of features to identify relationships:","metadata":{}},{"cell_type":"code","source":"\n# Separate features and target variable\nX = df.drop(columns=['target'])  # Features\ny = df['target']  # Target variable\nXpoly = df_poly.drop(columns=['target'])  # Features\nypoly = df_poly['target']  # Target variable\n\n# Step 1: Randomly select 10 variables from the original and polynomial DataFrames\nrandom_original_vars = X.sample(n=10, axis=1, random_state=42)\nrandom_polynomial_vars = Xpoly.sample(n=10, axis=1, random_state=42)\n\n# Step 2: Calculate the correlation matrices for the selected random variables\ncorrelation_original = random_original_vars.corr()\ncorrelation_polynomial = random_polynomial_vars.corr()\n\n# Step 3: Display the correlation matrices for the selected random variables\nprint(\"Correlation Matrix for 10 Random Original Features:\")\nprint(correlation_original)\n\nprint(\"\\nCorrelation Matrix for 10 Random Polynomial Features:\")\nprint(correlation_polynomial)\n\n# Plot the correlation matrix for original features\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_original, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\nplt.title(\"Correlation Matrix for 10 Random Original Features\")\nplt.show()\n\n# Plot the correlation matrix for polynomial features\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_polynomial, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\nplt.title(\"Correlation Matrix for 10 Random Polynomial Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:44:55.645449Z","iopub.execute_input":"2025-10-21T11:44:55.645748Z","iopub.status.idle":"2025-10-21T11:44:56.598347Z","shell.execute_reply.started":"2025-10-21T11:44:55.645721Z","shell.execute_reply":"2025-10-21T11:44:56.597398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"No obvious correlations or patterns","metadata":{}},{"cell_type":"markdown","source":"\n# PCA","metadata":{}},{"cell_type":"markdown","source":"## For PCA Change categorical input to numerical","metadata":{}},{"cell_type":"markdown","source":"Not relevant for this competition","metadata":{}},{"cell_type":"markdown","source":"# Split the dataset into train and test","metadata":{}},{"cell_type":"code","source":"train_ds_pd = dataset_df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:44:56.599616Z","iopub.execute_input":"2025-10-21T11:44:56.599946Z","iopub.status.idle":"2025-10-21T11:44:56.605100Z","shell.execute_reply.started":"2025-10-21T11:44:56.599918Z","shell.execute_reply":"2025-10-21T11:44:56.603974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Operate on training data","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"## Model: Tensor Flow Decision Forest TFDF","metadata":{}},{"cell_type":"code","source":"# convert the dataset from Pandas format (pd.DataFrame) into TensorFlow Datasets format (tf.data.Dataset).\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd)\n# valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)\n# train_ds = ydf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n# valid_ds = ydf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)\n# AttributeError: module 'ydf' has no attribute 'keras' aborting ydf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:44:56.606100Z","iopub.execute_input":"2025-10-21T11:44:56.606883Z","iopub.status.idle":"2025-10-21T11:44:56.955386Z","shell.execute_reply.started":"2025-10-21T11:44:56.606847Z","shell.execute_reply":"2025-10-21T11:44:56.954434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list the all the available models in TensorFlow Decision Forests\ntfdf.keras.get_all_models()\n# ydf.keras.get_all_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:44:56.956305Z","iopub.execute_input":"2025-10-21T11:44:56.956592Z","iopub.status.idle":"2025-10-21T11:44:56.962714Z","shell.execute_reply.started":"2025-10-21T11:44:56.956571Z","shell.execute_reply":"2025-10-21T11:44:56.961817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model: KNN","metadata":{}},{"cell_type":"markdown","source":"Hmmm... This reminds me that data preparation depends on modelling wishes... Maybe allow a little more flexibility in the approach","metadata":{}},{"cell_type":"markdown","source":"# Conclusion:\nIf there is a pattern in the data it is not obvious enough to be uncovered here. I will submit random results to the competition.","metadata":{}},{"cell_type":"code","source":"# Load sample submission into a Pandas Dataframe\nsample_submission = pd.read_csv('/kaggle/input/dont-overfit-ii/sample_submission.csv')\nprint(\"Full sample_submission shape is {}\".format(dataset_df.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:49:35.896669Z","iopub.execute_input":"2025-10-21T11:49:35.897016Z","iopub.status.idle":"2025-10-21T11:49:35.919046Z","shell.execute_reply.started":"2025-10-21T11:49:35.896991Z","shell.execute_reply":"2025-10-21T11:49:35.918050Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Make random values of 0 and 1 for target values:","metadata":{}},{"cell_type":"code","source":"# Replace values in 'target' with 0 or 1 completely at random\nsample_submission['target'] = np.random.randint(0, 2, size=len(sample_submission))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:58:33.560285Z","iopub.execute_input":"2025-10-21T11:58:33.560646Z","iopub.status.idle":"2025-10-21T11:58:33.568087Z","shell.execute_reply.started":"2025-10-21T11:58:33.560625Z","shell.execute_reply":"2025-10-21T11:58:33.567062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T11:58:42.918667Z","iopub.execute_input":"2025-10-21T11:58:42.919000Z","iopub.status.idle":"2025-10-21T11:58:42.929177Z","shell.execute_reply.started":"2025-10-21T11:58:42.918978Z","shell.execute_reply":"2025-10-21T11:58:42.927909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looks beautifully random. Save as submission.csv:","metadata":{}},{"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:00:50.889402Z","iopub.execute_input":"2025-10-21T12:00:50.889738Z","iopub.status.idle":"2025-10-21T12:00:50.915958Z","shell.execute_reply.started":"2025-10-21T12:00:50.889714Z","shell.execute_reply":"2025-10-21T12:00:50.915107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}